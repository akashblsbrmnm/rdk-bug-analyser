{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akashblsbrmnm/rdk-bug-analyser/blob/main/bug_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yE3Qt5bjFvlo"
      },
      "outputs": [],
      "source": [
        "!pip install --no-cache-dir numpy pandas faiss-cpu sentence-transformers transformers torch tqdm packaging openpyxl accelerate -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ozKjkdlGyYH"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"Please upload your Excel file with bug reports:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "excel_files = [f for f in uploaded.keys() if f.lower().endswith(('.xlsx', '.xls'))]\n",
        "if not excel_files:\n",
        "    raise ValueError(\"No valid Excel file found (.xlsx or .xls required)\")\n",
        "\n",
        "file_path = excel_files[0]\n",
        "print(f\"\\nSuccessfully uploaded: {file_path} ({len(uploaded[file_path])} bytes)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASqUxLatHQiD"
      },
      "outputs": [],
      "source": [
        "%%writefile bug_analyzer.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "import logging\n",
        "import re\n",
        "from typing import Dict, List, Any\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from packaging import version\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class BugAnalysis:\n",
        "    def __init__(self, excel_path: str,\n",
        "                 model_name: str = \"all-MiniLM-L6-v2\",\n",
        "                 llm_model_name: str = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "                 duplicate_threshold: float = 0.7,\n",
        "                 use_gpu: bool = False):\n",
        "\n",
        "        # Configure device\n",
        "        self.device = \"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\"\n",
        "        self.duplicate_threshold = duplicate_threshold\n",
        "        logger.info(f\"Using device: {self.device.upper()}\")\n",
        "\n",
        "        # Initialize embedding model\n",
        "        self.model = SentenceTransformer(model_name, device=self.device)\n",
        "\n",
        "        # Load bug data\n",
        "        self.bugs_df = self.load_excel_data(excel_path)\n",
        "        self.bug_embeddings = self.compute_embeddings()\n",
        "\n",
        "        # Initialize FAISS index\n",
        "        self.index = self._initialize_faiss_index()\n",
        "\n",
        "        # Initialize LLM\n",
        "        self.has_generator = False\n",
        "        self._initialize_llm(llm_model_name, use_gpu)\n",
        "\n",
        "    def _initialize_llm(self, model_name: str, use_gpu: bool) -> None:\n",
        "        \"\"\"Initialize LLM with fallback\"\"\"\n",
        "        try:\n",
        "            logger.info(f\"Initializing LLM: {model_name}\")\n",
        "            self.generator = pipeline(\n",
        "                \"text-generation\",\n",
        "                model=model_name,\n",
        "                device=0 if use_gpu else -1,\n",
        "                torch_dtype=torch.float16 if use_gpu else torch.float32\n",
        "            )\n",
        "            self.has_generator = True\n",
        "        except Exception as e:\n",
        "            logger.error(f\"LLM initialization failed: {str(e)}\")\n",
        "            self.has_generator = False\n",
        "\n",
        "    def load_excel_data(self, file_path: str) -> pd.DataFrame:\n",
        "        try:\n",
        "            df = pd.read_excel(file_path, engine=\"openpyxl\")\n",
        "            required_columns = {\n",
        "                \"bug_id\", \"bug_name\", \"bug_type\",\n",
        "                \"component_affected\", \"root_cause\", \"build_number\"\n",
        "            }\n",
        "\n",
        "            if not required_columns.issubset(df.columns):\n",
        "                missing = required_columns - set(df.columns)\n",
        "                raise ValueError(f\"Missing columns: {missing}\")\n",
        "\n",
        "            return df.fillna(\"\")\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Excel error: {str(e)}\")\n",
        "\n",
        "    def _initialize_faiss_index(self) -> faiss.Index:\n",
        "        \"\"\"Create FAISS index with cosine similarity\"\"\"\n",
        "        index = faiss.IndexFlatIP(self.bug_embeddings.shape[1])\n",
        "        embeddings = self.bug_embeddings.astype('float32')\n",
        "        faiss.normalize_L2(embeddings)\n",
        "        index.add(embeddings)\n",
        "        return index\n",
        "\n",
        "    def compute_embeddings(self) -> np.ndarray:\n",
        "        \"\"\"Generate and normalize embeddings\"\"\"\n",
        "        bug_texts = self.bugs_df.apply(\n",
        "            lambda row: f\"{row['bug_name']} {row['bug_type']} {row['component_affected']}\",\n",
        "            axis=1\n",
        "        ).tolist()\n",
        "        embeddings = self.model.encode(bug_texts, convert_to_numpy=True, show_progress_bar=True)\n",
        "        faiss.normalize_L2(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "    def analyze_bug(self, new_bug: Dict[str, str]) -> Dict[str, Any]:\n",
        "      \"\"\"Full analysis workflow\"\"\"\n",
        "      query_text = f\"{new_bug['bug_name']} {new_bug['bug_type']} {new_bug['component_affected']}\"\n",
        "      query_embedding = self.model.encode([query_text])\n",
        "      faiss.normalize_L2(query_embedding)\n",
        "\n",
        "      D, I = self.index.search(query_embedding.astype('float32'), 5)\n",
        "\n",
        "      similar_bugs = []\n",
        "      for score, idx in zip(D[0], I[0]):\n",
        "          if idx < len(self.bugs_df):\n",
        "              bug = self.bugs_df.iloc[idx].to_dict()\n",
        "              bug[\"score\"] = float(score)\n",
        "              similar_bugs.append(bug)\n",
        "\n",
        "      sorted_similar_bugs = sorted(similar_bugs, key=lambda x: x[\"score\"], reverse=True)\n",
        "\n",
        "      return {\n",
        "          \"similar_bugs\": sorted_similar_bugs,\n",
        "          \"is_duplicate\": any(b[\"score\"] >= self.duplicate_threshold for b in similar_bugs),\n",
        "          \"root_causes\": self._analyze_root_causes(similar_bugs),\n",
        "          \"explanation\": self._generate_explanation(similar_bugs, new_bug)\n",
        "      }\n",
        "\n",
        "    def _analyze_root_causes(self, similar_bugs: List[Dict[str, Any]]) -> List[tuple]:\n",
        "      \"\"\"Weighted root cause analysis\"\"\"\n",
        "      causes = defaultdict(float)\n",
        "      total_score = sum(bug['score'] for bug in similar_bugs)\n",
        "\n",
        "      for bug in similar_bugs:\n",
        "          if bug['root_cause'] and total_score > 0:\n",
        "              causes[bug['root_cause']] += bug['score'] / total_score\n",
        "\n",
        "      return sorted(causes.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "\n",
        "    def _generate_explanation(self, similar_bugs: List[Dict], new_bug: Dict) -> str:\n",
        "        \"\"\"Generate LLM explanation\"\"\"\n",
        "        if not self.has_generator or not similar_bugs:\n",
        "            return \"Explanation unavailable\"\n",
        "\n",
        "        context = \"\\n\".join([f\"- {b['score']:.2f}: {b['bug_name']} ({b['root_cause']})\" for b in similar_bugs[:3]])\n",
        "        prompt = f\"\"\"Analyze this new bug:\n",
        "        Name: {new_bug['bug_name']}\n",
        "        Type: {new_bug['bug_type']}\n",
        "        Component: {new_bug['component_affected']}\n",
        "\n",
        "        Similar bugs:\n",
        "        {context}\n",
        "\n",
        "        Technical analysis of likely root causes:\"\"\"\n",
        "\n",
        "        try:\n",
        "            result = self.generator(prompt, max_length=200)[0]['generated_text']\n",
        "            return result.split(\"Technical analysis\")[-1].strip()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Explanation failed: {str(e)}\")\n",
        "            return \"Could not generate explanation\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiQ_9rpVHf9W"
      },
      "outputs": [],
      "source": [
        "from bug_analyzer import BugAnalysis\n",
        "\n",
        "# Initialize with GPU support\n",
        "analyzer = BugAnalysis(\n",
        "    excel_path=file_path,\n",
        "    llm_model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    use_gpu=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lt4wQkBjHj-t"
      },
      "outputs": [],
      "source": [
        "\n",
        "new_bug = {\n",
        "    \"bug_name\": \"device failing to start\",\n",
        "    \"bug_type\": \"crash\",\n",
        "    \"component_affected\": \"core\"\n",
        "}\n",
        "\n",
        "results = analyzer.analyze_bug(new_bug)\n",
        "\n",
        "print(\"\\nAnalysis Results:\")\n",
        "print(f\"Duplicate: {'Yes' if results['is_duplicate'] else 'No'}\")\n",
        "print(\"\\nTop Similar Bugs:\")\n",
        "for bug in results['similar_bugs']:\n",
        "    print(f\"- {bug['bug_id']} ({bug['score']:.2f}): {bug['root_cause']}\")\n",
        "\n",
        "print(\"\\nRoot Cause Probabilities:\")\n",
        "for cause, prob in results['root_causes']:\n",
        "    print(f\"- {cause}: {prob*100:.1f}%\")\n",
        "\n",
        "print(\"\\nExplanation:\", results['explanation'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3Y9b3-0Hn8M"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# import json\n",
        "# from datetime import datetime\n",
        "\n",
        "# drive.mount('/content/drive')\n",
        "# timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# save_path = f\"/content/drive/MyDrive/bug_analysis_{timestamp}.json\"\n",
        "# with open(save_path, 'w') as f:\n",
        "#     json.dump(results, f, indent=2)\n",
        "\n",
        "# print(f\"âœ… Results saved to: {save_path}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOglSwsY9uVl2eq9YNiADvk",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
